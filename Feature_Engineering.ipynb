{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObPN5C3n3hrmL6P2m/6stX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siddhanttambe2/Feature-Engineering/blob/main/Feature_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) What is a parameter?"
      ],
      "metadata": {
        "id": "1uAwwHN2U8bB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "parameters are the internal configurations of a model that are learned from the data during training. These values are adjusted iteratively as the model processes data, optimizing for a specific objective, like minimizing prediction error. Parameters define the behavior of the model and how it transforms inputs into outputs."
      ],
      "metadata": {
        "id": "Zp3s3JkfVswH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is correlation?\n",
        "What does negative correlation mean?"
      ],
      "metadata": {
        "id": "5aDN8OFDWdFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation is a statistical measure that describes the degree to which two variables are related or move together. In machine learning, correlation helps in understanding the relationships between input features (independent variables) and the target variable (dependent variable), as well as among the features themselves.\n",
        "\n",
        "The correlation is quantified using a correlation coefficient, typically ranging between -1 and 1:\n",
        "\n",
        "+1: Perfect positive correlation (variables move in the same direction).\n",
        "\n",
        "0: No correlation (no relationship).\n",
        "\n",
        "-1: Perfect negative correlation (variables move in opposite directions)\n",
        "\n",
        "Negative Correlation:\n",
        "Negative correlation occurs when one variable increases while the other decreases. It indicates an inverse relationship.\n",
        "\n",
        "Example:\n",
        "In a dataset studying temperature and heater usage:\n",
        "As temperature increases, heater usage tends to decrease. This is a negative correlation.\n"
      ],
      "metadata": {
        "id": "0PI_qbOWWIlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "4OR4mGG_Wywi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning is a branch of artificial intelligence (AI) that enables systems to learn and improve from experience without being explicitly programmed. It involves developing algorithms that allow computers to identify patterns in data and make predictions or decisions based on it.\n",
        "\n",
        "Data\n",
        "\n",
        "The raw material for machine learning. It can include structured data (e.g., spreadsheets, databases) and unstructured data (e.g., text, images).\n",
        "\n",
        "Features (Input Variables)\n",
        "\n",
        "The measurable properties or characteristics of the data that the model uses to learn.\n",
        "\n",
        "Model\n",
        "\n",
        " A mathematical representation or algorithm that learns patterns from data.\n",
        "Types: Linear regression, decision trees, neural networks, etc.\n",
        "\n",
        "Training\n",
        "\n",
        "The process of teaching the model to recognize patterns by feeding it labeled or unlabeled data.\n",
        "\n",
        "Loss Function\n",
        "\n",
        "A mathematical function that measures the difference between the model's prediction and the actual outcome.\n",
        "\n",
        "Optimization Algorithm\n",
        "\n",
        "An algorithm (e.g., gradient descent) that adjusts the model's parameters to minimize the loss function.\n",
        "\n",
        "Evaluation\n",
        "\n",
        "The process of testing the  model on unseen data to assess its performance.\n",
        "Metrics: Accuracy, precision, recall, F1 score, etc.\n",
        "\n",
        "Prediction\n",
        "\n",
        "Using the trained model to infer or predict outcomes on new, unseen data.\n",
        "Importance: The ultimate goal of machine learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "nKdUQzKXW6ZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.How does loss value help in determining whether the model is good or not?\n"
      ],
      "metadata": {
        "id": "79Ra3RGPB0bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss value measures how well a machine learning model's predictions align with the actual outcomes, with lower loss indicating better performance. During training, the model's parameters are adjusted to minimize the loss function, improving accuracy and generalization. A consistently high loss suggests the model is underperforming due to issues like underfitting, poor feature selection, or insufficient training, while a very low loss on training data but high loss on validation data indicates overfitting. Monitoring loss values helps assess and refine the model's quality.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gVg2DgIzCKso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "lDJGp7CwCPSJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous Variables\n",
        "\n",
        "Definition: Variables that can take any numerical value within a range and have an infinite number of possible values.\n",
        "Examples: Age, height, temperature, salary, etc.\n",
        "\n",
        "Categorical Variables\n",
        "\n",
        "Definition: Variables that represent distinct groups or categories with a finite number of values.\n",
        "Examples: Gender (Male/Female), color (Red/Blue/Green), payment method (Cash/Card)."
      ],
      "metadata": {
        "id": "20cQcnq5Cabf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.How do we handle categorical variables in Machine Learning? What are the common techniques?"
      ],
      "metadata": {
        "id": "v-iCIpVOCtxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In machine learning, categorical variables need to be converted into numerical values for the model to process them effectively. Common techniques for handling categorical variables include One-Hot Encoding, where each category is represented as a binary vector; Label Encoding, where categories are assigned unique integer labels; and Ordinal Encoding, used when categories have an inherent order (e.g., low, medium, high). Other techniques, like Target Encoding, replace categories with the mean of the target variable, and Binary Encoding is used for high-cardinality categorical variables. The choice of method depends on the nature of the categorical variable and the model being used"
      ],
      "metadata": {
        "id": "9ROIJbeSDGJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.What do you mean by training and testing a dataset?"
      ],
      "metadata": {
        "id": "6eoCI8VFDOp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and testing a dataset are essential steps in machine learning to evaluate how well a model performs.\n",
        "\n",
        "Training a Dataset: This involves using a portion of the data (the training set) to teach the model. During training, the model learns patterns, relationships, and structures in the data by adjusting its parameters to minimize errors. The training dataset is used to fit the model, allowing it to make predictions and optimize its performance.\n",
        "\n",
        "Testing a Dataset: After training, the model is evaluated on a separate portion of the data (the testing set) that it has never seen before. This helps assess how well the model generalizes to new, unseen data. The testing dataset measures the model's accuracy, robustness, and ability to make correct predictions in real-world scenarios."
      ],
      "metadata": {
        "id": "aUQ_o7iFDWd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "M-jTpBmoEGj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.preprocessing is a module in the scikit-learn library that provides various functions to preprocess data before feeding it into machine learning models. It includes techniques for scaling, transforming, and encoding data to make it suitable for modeling. The goal of preprocessing is to enhance the model's performance and accuracy by ensuring that the data is in the right format and range."
      ],
      "metadata": {
        "id": "sfj1Ua7QEOXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.What is a Test set?"
      ],
      "metadata": {
        "id": "s7etL4VYEXT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A test set is a subset of the dataset that is used to evaluate the performance of a machine learning model after it has been trained. It consists of data that the model has never seen during the training process, ensuring an unbiased assessment of its ability to generalize to new, unseen data. By testing the model on the test set, we can measure its accuracy, robustness, and how well it can make predictions on real-world data. The test set helps identify issues like overfitting, where the model performs well on training data but poorly on new data."
      ],
      "metadata": {
        "id": "fNVo8NoOEcwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "yAUAH31lEn8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To split data for model fitting (training and testing) in Python, the train_test_split function from the scikit-learn library is commonly used. This function randomly splits the dataset into two parts: one for training the model and one for testing it. Typically, a common split ratio is 80% for training and 20% for testing, though this can vary depending on the size of the dataset. You can also stratify the split to maintain the proportion of target classes, which is useful in imbalanced datasets.\n",
        "\n",
        "When approaching a machine learning problem, the process generally follows several key steps: 1) Understand the problem and define the objectives. 2) Collect and preprocess data, which includes cleaning, transforming, and encoding features as necessary. 3) Split the data into training and testing sets. 4) Choose an appropriate model based on the problem type (e.g., regression, classification). 5) Train the model using the training data, optimizing it with techniques like cross-validation or hyperparameter tuning. 6) Evaluate the model's performance on the test data, using metrics like accuracy, precision, or mean squared error. 7) Iterate and improve by refining the model, experimenting with different algorithms, or adjusting features based on performance"
      ],
      "metadata": {
        "id": "4fTqblTfEy16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "9G76K8jlE7Dn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing Exploratory Data Analysis (EDA) before fitting a model to the data is crucial because it helps in understanding the dataset and making informed decisions about data preprocessing, feature selection, and model choice. EDA allows you to identify patterns, relationships, and potential issues in the data, such as missing values, outliers, or inconsistencies, that may negatively affect model performance. By visualizing and summarizing the data, you can gain insights into the distribution of features, detect correlations, and understand the structure of the target variable. This understanding guides decisions like whether to scale features, encode categorical variables, or handle imbalanced classes. Overall, EDA helps improve the quality of the data, ensuring the model is trained on well-prepared data and thereby enhancing its accuracy and generalizability."
      ],
      "metadata": {
        "id": "ZHqm7Sq1FBTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.What is correlation?"
      ],
      "metadata": {
        "id": "8qY54qK4Fh8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation is a statistical measure that describes the relationship between two or more variables, indicating how they move in relation to each other. It quantifies the strength and direction of the association between the variables. The correlation coefficient, typically ranging from -1 to 1, measures this relationship:\n",
        "\n",
        "+1: Perfect positive correlation (both variables move in the same direction).\n",
        "\n",
        "-1: Perfect negative correlation (one variable increases as the other decreases).\n",
        "\n",
        "0: No correlation (no linear relationship between the variables).\n",
        "\n",
        "Correlation helps in understanding whether changes in one variable can predict changes in another. In machine learning, understanding the correlation between features and the target variable is crucial for feature selection and model optimization."
      ],
      "metadata": {
        "id": "VQxzTPWeFpsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.What does negative correlation mean?"
      ],
      "metadata": {
        "id": "cvK55pxmGJcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative correlation means that as one variable increases, the other decreases, or vice versa. In other words, the two variables move in opposite directions. The correlation coefficient for negative correlation ranges from 0 to -1, where:\n",
        "A correlation coefficient of -1 indicates a perfect negative correlation, meaning that for every unit increase in one variable, the other decreases by a consistent amount.\n",
        "A correlation coefficient closer to 0 indicates a weaker negative relationship, meaning the variables are less inversely related.\n",
        "For example, in a dataset where the number of hours worked increases, the number of hours spent on leisure activities might decrease, showing a negative correlation. This type of relationship is often observed in cases of trade-offs or inverse dependencies between two variables."
      ],
      "metadata": {
        "id": "5l1Fah-5GUX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.How can you find correlation between variables in Python?"
      ],
      "metadata": {
        "id": "omu976IzGc6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, you can find the correlation between variables using the pandas library, which provides the corr() function. First, load your dataset into a pandas DataFrame, then call df.corr(), where df is the DataFrame. This function computes the Pearson correlation coefficient between each pair of numeric variables in the dataset, returning a correlation matrix. You can also use Seaborn to visualize the correlation matrix with a heatmap, using sns.heatmap(df.corr(), annot=True) for easy interpretation. For non-linear relationships or specific correlation types, you can explore other methods, like Spearman or Kendall correlation, by passing the method argument to corr(). This helps in understanding the strength and direction of relationships between variables."
      ],
      "metadata": {
        "id": "knGhvpexGnn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.What is causation? Explain difference between correlation and causation with an example."
      ],
      "metadata": {
        "id": "sR-Ue_4FHPX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Causation refers to a cause-and-effect relationship between two variables, where a change in one variable directly causes a change in another. In other words, one event (the cause) leads to the outcome (the effect). Causation implies that there is a direct mechanism through which one variable influences the other.\n",
        "\n",
        "Difference Between Correlation and Causation:\n",
        "\n",
        "Correlation: Describes the statistical relationship between two variables, where they tend to vary together (either positively or negatively), but it does not imply that one variable causes the other. Correlation merely indicates a pattern of co-occurrence between variables.\n",
        "\n",
        "Causation: Implies that one variable has a direct influence on another. It establishes a cause-effect relationship, meaning that a change in one variable directly results in a change in the other.\n",
        "\n",
        "Example:\n",
        "\n",
        "Correlation: There is a correlation between the number of ice creams sold and the number of people who drown. Both tend to increase during the summer months, but this does not mean that buying ice cream causes drowning. Instead, both are related to the warmer weather, which is the true underlying factor.\n",
        "\n",
        "Causation: Smoking causes lung cancer. The act of smoking leads directly to the development of lung cancer, which is a clear cause-and-effect relationship."
      ],
      "metadata": {
        "id": "VLCXqpWjHaJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.What is an Optimizer? What are different types of optimizers? Explain each with an example.\n"
      ],
      "metadata": {
        "id": "tZYe1fE2Hr7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An **optimizer** in machine learning is an algorithm used to adjust a model's parameters (e.g., weights) to minimize the loss function and improve performance. Optimizers work by updating parameters iteratively based on the gradients calculated during backpropagation. Common types of optimizers include **Gradient Descent (GD)**, which updates weights based on the entire dataset and moves in the direction of the negative gradient; **Stochastic Gradient Descent (SGD)**, which updates weights using one data point at a time, making it faster but noisier; and **Mini-Batch Gradient Descent**, a hybrid of GD and SGD that processes small subsets of data for balanced efficiency and stability. Advanced optimizers like **Adam (Adaptive Moment Estimation)** combine momentum and adaptive learning rates, making training efficient for large and complex datasets. **RMSprop** adapts learning rates for each parameter, maintaining steady convergence, while **Momentum** accelerates optimization by incorporating past gradients to smooth updates. For instance, Adam is widely used in deep learning as it adapts effectively to varying data distributions and requires minimal tuning. Each optimizer has strengths and is chosen based on the dataset, problem, and model type."
      ],
      "metadata": {
        "id": "0eGFB-GNtw7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.What is sklearn.linear_model ?"
      ],
      "metadata": {
        "id": "TTdqi7WeunpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.linear_model is a module within the scikit-learn library in Python that provides a comprehensive suite of linear models for both regression and classification tasks. This module includes a variety of algorithms that assume a linear relationship between the input features and the target variable. Key functionalities offered by sklearn.linear_model include Linear Regression, Logistic Regression, Ridge Regression, Lasso Regression, Elastic Net, and Support Vector Machines (SVM) for linear classification and regression. These models are essential for tasks where interpretability and simplicity are important, as they allow for understanding the influence of each feature on the prediction. Additionally, the module supports regularization techniques (like L1 and L2 regularization) to prevent overfitting by penalizing large coefficients. For example, LinearRegression can be used to predict continuous outcomes by fitting a linear equation to the observed data, while LogisticRegression is employed for binary or multiclass classification problems by modeling the probability of class membership. The sklearn.linear_model module is highly versatile and integrates seamlessly with other scikit-learn tools, making it a fundamental component for building and evaluating linear-based machine learning models."
      ],
      "metadata": {
        "id": "WWWf7M9QuGwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "88sah6rSuKWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model.fit() method in machine learning is used to train a model on a given dataset. It adjusts the model's internal parameters (e.g., weights) based on the input data and the target output to minimize the loss function and make accurate predictions. This is the core step in building a machine learning model, as it \"fits\" the model to the data.\n",
        "\n",
        "Arguments for fit():\n",
        "\n",
        "X (Features): The input data or features used for training, typically a 2D array-like structure where rows represent samples and columns represent features (e.g., X_train in supervised learning).\n",
        "\n",
        "y (Target): The target variable or labels corresponding to the input features, a 1D array-like structure for regression or classification tasks (e.g., y_train)."
      ],
      "metadata": {
        "id": "t3YD4cRvubXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.What does model.predict() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "wOEQBiYquwH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model.predict() method in machine learning is used to make predictions on new, unseen data after a model has been trained using the fit() method. It applies the learned parameters (e.g., weights, biases) to the input data and outputs the predicted values, which could be continuous (for regression) or categorical labels/probabilities (for classification).\n",
        "\n",
        "Arguments for predict():\n",
        "\n",
        "X (Features): The input data for which predictions are to be made. This is typically a 2D array-like structure (e.g., X_test), where rows represent samples and columns represent features. The shape of X must match the feature structure used during training."
      ],
      "metadata": {
        "id": "Fjmrdbr-u7do"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "u7wSX22tvFR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous Variables:\n",
        "\n",
        "These are numeric variables that can take any value within a range, including fractions and decimals. They represent measurable quantities.\n",
        "Examples: Height (in cm), weight (in kg), temperature (in °C), and age (in years).\n",
        "\n",
        "Usage in ML: Continuous variables are often used as features in regression models or as predictors in other models. They require normalization or standardization when working with distance-based algorithms (e.g., KNN or SVM).\n",
        "\n",
        "Categorical Variables:\n",
        "\n",
        "These are variables that represent distinct groups or categories. They can be either:\n",
        "Nominal: Categories without any specific order (e.g., colors: red, blue, green).\n",
        "Ordinal: Categories with a meaningful order but no fixed spacing between them (e.g., education levels: high school, bachelor's, master's).\n",
        "Examples: Gender (male/female), city names, product types, and customer satisfaction ratings.\n",
        "\n",
        "Usage in ML: Categorical variables often require encoding techniques, like one-hot encoding or label encoding, to be converted into numeric formats that machine learning models can understand.\n"
      ],
      "metadata": {
        "id": "1oT55F13vPgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.What is feature scaling? How does it help in Machine Learning?"
      ],
      "metadata": {
        "id": "eQCR-MUqvgFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling is the process of normalizing or standardizing the range of features (input variables) in a dataset to ensure they are on a comparable scale. It involves transforming the values of features to fall within a specific range, such as [0, 1], or adjusting them to have a mean of 0 and a standard deviation of 1."
      ],
      "metadata": {
        "id": "bT_-SpdPvpV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.How do we perform scaling in Python?"
      ],
      "metadata": {
        "id": "zsbExwIgvxZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, scaling is typically performed using the scikit-learn library, which provides various preprocessing tools for feature scaling. The two most common methods are StandardScaler (standardization) and MinMaxScaler (normalization). First, you import the scaler of choice from sklearn.preprocessing. Then, you fit the scaler to the data using the fit() method and transform the data with transform(), or you can combine both steps with fit_transform(). For example, to standardize data, you use StandardScaler to scale the features to have a mean of 0 and a standard deviation of 1. Similarly, MinMaxScaler can normalize data to a specific range, such as [0, 1]. These methods ensure all features contribute equally to the model, improving training stability and performance."
      ],
      "metadata": {
        "id": "eHihuhOTwHTw"
      }
    },
    {
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "pwMAJnW5wXCe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "Q24OwUBLwhu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.preprocessing is a module in the scikit-learn library that provides various tools for preprocessing and transforming data before training a machine learning model. Preprocessing involves cleaning, scaling, and encoding features to ensure they are in a suitable format for modeling. The module includes methods for feature scaling (e.g., StandardScaler, MinMaxScaler), encoding categorical variables (e.g., LabelEncoder, OneHotEncoder), handling missing values (e.g., SimpleImputer), and feature generation (e.g., PolynomialFeatures). These tools help standardize input data, normalize ranges, encode non-numeric data, and address outliers or irregularities, ensuring better performance and convergence of machine learning models. For instance, StandardScaler standardizes data to have a mean of 0 and standard deviation of 1, while OneHotEncoder converts categorical features into a binary matrix. This module simplifies the preprocessing pipeline, ensuring data is ready for training efficiently"
      ],
      "metadata": {
        "id": "rNftpX_Cwl8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "0BH6VedRwptl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, you can split data for model fitting using train_test_split() from the sklearn.model_selection module. This function divides the dataset into two parts: a training set and a testing set. You typically specify the proportion of data to be used for training (e.g., 80%) and testing (e.g., 20%)"
      ],
      "metadata": {
        "id": "vEvufqwtw79F"
      }
    },
    {
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = [[1, 2], [3, 4], [5, 6], [7, 8]]\n",
        "y = [0, 1, 0, 1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "hzlmQyogxCgS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.Explain data encoding?"
      ],
      "metadata": {
        "id": "mj4hViKwxM2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Data encoding is the process of converting categorical data into a numerical format that machine learning models can understand. Many algorithms require input features to be numeric, and encoding transforms non-numeric variables, such as strings or labels, into numeric representations. Common encoding techniques include Label Encoding, where each category is assigned a unique integer (e.g., 'Red' becomes 1, 'Blue' becomes 2), and One-Hot Encoding, where each category is transformed into a binary vector (e.g., 'Red' becomes [1, 0, 0] for three possible colors). Ordinal Encoding is used when categories have an inherent order (e.g., 'Low', 'Medium', 'High'), where each category is mapped to an integer reflecting its rank. Encoding is crucial for preparing data for machine learning algorithms, ensuring that categorical variables are represented in a form that the model can process effectively."
      ],
      "metadata": {
        "id": "ZqhvdwczxWCM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "whF7g-mJw-Ey"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}